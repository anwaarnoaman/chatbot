ğŸš€ Project Setup & Execution Guide (Python 3.13.5)

ğŸ 1. Run Locally with Virtual Environment (No Docker)

âœ… Step 1: Create and activate virtual environment
From the project root:

python3.13 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
âœ… Step 2: Install dependencies
pip install --upgrade pip
pip install -r requirements.txt
âœ… Step 3: Run backend and frontend
ğŸ”§ Backend (FastAPI):

cd inference_api
uvicorn app:app --reload --host 0.0.0.0 --port 8063
ğŸ’¬ Frontend (Streamlit):

Open another terminal/tab:

cd chat_app
streamlit run app.py
ğŸ§ª 2. Run Backend and Frontend Separately (Without Docker)

Use this if dependencies are already available locally.

âœ… Backend (FastAPI)
cd inference_api
uvicorn app:app --reload --host 0.0.0.0 --port 8063
âœ… Frontend (Streamlit)
cd chat_app
streamlit run app.py
ğŸ³ 3. Run Backend and Frontend Separately with Docker (Without Compose)

Build and run each service manually.

ğŸ”§ Backend (inference_api)
cd inference_api
docker build -t inference_api -f Dockerfile.inference_api .
docker run -p 8063:8063 inference_api
ğŸ’¬ Frontend (chat_app)
In a new terminal/tab:

cd chat_app
docker build -t chat_app -f Dockerfile.chat_app .
docker run -p 8501:8501 chat_app
ğŸ“¦ 4. Run the Full App with Docker Compose

This runs both backend and frontend together with isolated networking.

â–¶ï¸ Start the full stack:
From the project root:

docker compose up --build
ğŸ”— FastAPI backend: http://localhost:8063
ğŸ”— Streamlit frontend: http://localhost:8501
â¹ï¸ To stop everything:
docker compose down
âš™ï¸ Environment Configuration Notes

When running locally (venv/manual):
Ensure chat_app connects to:
http://localhost:8063
When using Docker Compose:
The frontend should connect to the backend via:
http://inference_api:8063
(Docker Compose sets up internal DNS-based networking.)
