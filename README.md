ğŸš€ Project Setup & Execution Guide (Python 3.13.5)



Note: Inference Api 


https://avrioc_inference.jhingaai.com/



ğŸ 1. Run Locally with Virtual Environment (No Docker)

âœ… Step 1: Create and activate virtual environment
From the project root:

python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
âœ… Step 2: Install dependencies
pip install --upgrade pip
pip install -r requirements.txt
âœ… Step 3: Run backend and frontend

ğŸ§ª 2. Run Backend and Frontend Separately (Without Docker)
 
ğŸ”§ Backend (FastAPI):

uvicorn inference_api.app:app --reload --host 0.0.0.0 --port 8063

ğŸ’¬ Frontend (Streamlit):

Open another terminal/tab:
 
streamlit run chat_app/app.py

ğŸ³ 3. Run Backend and Frontend Separately with Docker (Without Compose)

Build and run each service manually.

ğŸ”§ Backend (inference_api)
 
sudo docker build -t inference_api -f Dockerfile.inference_api .
sudo docker run -p 8063:8063 inference_api
ğŸ’¬ Frontend (chat_app)
In a new terminal/tab:

 
sudo docker build -t chat_app -f Dockerfile.chat_app .
sudo docker run -p 8501:8501 chat_app
ğŸ“¦ 4. Run the Full App with Docker Compose

This runs both backend and frontend together with isolated networking.

â–¶ï¸ Start the full stack:
From the project root:

sudo docker compose up --build
ğŸ”— FastAPI backend: http://localhost:8063
ğŸ”— Streamlit frontend: http://localhost:8501
â¹ï¸ To stop everything:
sudo docker compose down
âš™ï¸ Environment Configuration Notes




When running locally (venv/manual):
Ensure chat_app connects to:

cloud infrence endpoint 

https://avrioc_inference.jhingaai.com/

if running locally 
http://localhost:8063


When using Docker Compose:
The frontend should connect to the backend via:
http://inference_api:8063


(Docker Compose sets up internal DNS-based networking.)


 